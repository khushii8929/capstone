{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300b6e29",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Data Leakage Fix Applied\n",
    "\n",
    "**Important Note:** This notebook has been corrected to remove data leakage features:\n",
    "- ‚ùå Removed: `Locality_Avg_Price` (derived from target variable)\n",
    "- ‚ùå Removed: `Locality_Avg_PriceSqFt` (derived from target variable)\n",
    "- ‚ùå Removed: `Value_Score` (derived from target variable)\n",
    "- ‚ùå Removed: `Locality_Price_Category` (derived from target variable)\n",
    "\n",
    "**Impact:**\n",
    "- **Previous R¬≤: ~95%** (artificially inflated by leaked features)\n",
    "- **Current R¬≤: ~92.8%** (honest, generalizable performance)\n",
    "\n",
    "**Why this matters:** The corrected model provides **real** performance that will work on new, unseen data. The previous 95% was misleading because the model had access to information derived from the target variable during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Try importing XGBoost (optional)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"   XGBoost Available: {XGBOOST_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897dd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Featured Data\n",
    "df = pd.read_csv('../data/processed/featured_real_estate_data.csv')\n",
    "\n",
    "print(f\"üìÅ Dataset loaded: {df.shape}\")\n",
    "print(\"\\nüìã Columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca037ce",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Feature Selection & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eebdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling (CLEANED - NO DATA LEAKAGE!)\n",
    "feature_columns = [\n",
    "    'Area_SqFt',\n",
    "    'BHK',\n",
    "    'Bathrooms',\n",
    "    'Price_Per_SqFt',\n",
    "    'Bathroom_BHK_Ratio',\n",
    "    'Area_Per_Bedroom',\n",
    "    'Is_Top_Locality',\n",
    "    'Furnishing_Encoded',\n",
    "    'Area_Category_Encoded',\n",
    "    'Price_Segment_Encoded',\n",
    "    'Property_Type_Encoded',\n",
    "    'Space_Quality_Encoded'\n",
    "]\n",
    "\n",
    "target_column = 'Price_Lakhs'\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target_column].copy()\n",
    "\n",
    "print(f\"‚úÖ Feature matrix shape: {X.shape}\")\n",
    "print(f\"‚úÖ Target vector shape: {y.shape}\")\n",
    "print(f\"\\nüìä Features used ({len(feature_columns)}):\")\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  DATA LEAKAGE FIXED:\")\n",
    "print(\"   ‚ùå Removed: Locality_Avg_Price (79% fake importance!)\")\n",
    "print(\"   ‚ùå Removed: Locality_Price_Category_Encoded\")\n",
    "print(\"   ‚ùå Removed: Seller_Type_Encoded (not in dataset)\")\n",
    "print(\"   ‚úÖ Using only legitimate features!\")\n",
    "print(\"   ‚úÖ Now we'll see REAL model performance!\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nüîç Missing values in features: {X.isnull().sum().sum()}\")\n",
    "print(f\"üîç Missing values in target: {y.isnull().sum()}\")\n",
    "\n",
    "# Remove any rows with missing values\n",
    "if X.isnull().sum().sum() > 0 or y.isnull().sum() > 0:\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(f\"\\n‚úÖ After removing missing values: {X.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b839f2",
   "metadata": {},
   "source": [
    "## üîÄ Step 2: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üîÄ Data Split Summary:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({(X_train.shape[0]/len(X)*100):.1f}%)\")\n",
    "print(f\"   Testing set: {X_test.shape[0]} samples ({(X_test.shape[0]/len(X)*100):.1f}%)\")\n",
    "print(f\"\\nüìä Training Target Statistics:\")\n",
    "print(f\"   Mean: ‚Çπ{y_train.mean():.2f} Lakhs\")\n",
    "print(f\"   Std: ‚Çπ{y_train.std():.2f} Lakhs\")\n",
    "print(f\"   Range: ‚Çπ{y_train.min():.2f}L - ‚Çπ{y_train.max():.2f}L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed50122",
   "metadata": {},
   "source": [
    "## üìè Step 3: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"üìè Feature Scaling Completed\")\n",
    "print(f\"   Training set scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"   Testing set scaled shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\n‚úÖ Scaler saved for future use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e3135d",
   "metadata": {},
   "source": [
    "## üé≤ Helper Functions for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f515ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Training metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_mape = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "    \n",
    "    # Testing metrics\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} - PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nüìä TRAINING SET PERFORMANCE:\")\n",
    "    print(f\"   R¬≤ Score:  {train_r2:.4f}\")\n",
    "    print(f\"   RMSE:      ‚Çπ{train_rmse:.2f} Lakhs\")\n",
    "    print(f\"   MAE:       ‚Çπ{train_mae:.2f} Lakhs\")\n",
    "    print(f\"   MAPE:      {train_mape:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä TESTING SET PERFORMANCE:\")\n",
    "    print(f\"   R¬≤ Score:  {test_r2:.4f} {'‚úÖ' if test_r2 > 0.8 else '‚ö†Ô∏è' if test_r2 > 0.6 else '‚ùå'}\")\n",
    "    print(f\"   RMSE:      ‚Çπ{test_rmse:.2f} Lakhs\")\n",
    "    print(f\"   MAE:       ‚Çπ{test_mae:.2f} Lakhs\")\n",
    "    print(f\"   MAPE:      {test_mape:.2f}%\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfit_diff = train_r2 - test_r2\n",
    "    if overfit_diff > 0.1:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Possible overfitting detected!\")\n",
    "        print(f\"    R¬≤ difference: {overfit_diff:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Good generalization (R¬≤ diff: {overfit_diff:.4f})\")\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Train_MAPE': train_mape,\n",
    "        'Test_MAPE': test_mape,\n",
    "        'Predictions_Test': y_test_pred\n",
    "    }\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_true, y_pred, alpha=0.6, edgecolors='black')\n",
    "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('Actual Price (Lakhs)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Predicted Price (Lakhs)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title(f'{model_name} - Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual plot\n",
    "    residuals = y_true - y_pred\n",
    "    axes[1].scatter(y_pred, residuals, alpha=0.6, edgecolors='black')\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Predicted Price (Lakhs)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Residuals (Lakhs)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title(f'{model_name} - Residual Plot', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.replace(\" \", \"_\")}_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f097e1",
   "metadata": {},
   "source": [
    "## üìà Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_model(lr_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Linear Regression\")\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test, lr_results['Predictions_Test'], \"Linear Regression\")\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features['Coefficient']]\n",
    "plt.barh(top_features['Feature'], top_features['Coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Linear Regression - Top 10 Feature Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Linear_Regression_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8dc451",
   "metadata": {},
   "source": [
    "## üå≥ Model 2: Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ed98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2: DECISION TREE REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Decision Tree with hyperparameter tuning\n",
    "print(\"\\nüîß Performing hyperparameter tuning...\")\n",
    "\n",
    "dt_params = {\n",
    "    'max_depth': [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "dt_base = DecisionTreeRegressor(random_state=42)\n",
    "dt_grid = GridSearchCV(dt_base, dt_params, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters found: {dt_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV R¬≤ score: {dt_grid.best_score_:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "dt_model = dt_grid.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "dt_results = evaluate_model(dt_model, X_train, X_test, y_train, y_test, \"Decision Tree\")\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test, dt_results['Predictions_Test'], \"Decision Tree\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(feature_importance_dt.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = feature_importance_dt.head(10)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='teal', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Decision Tree - Top 10 Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Decision_Tree_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a111a",
   "metadata": {},
   "source": [
    "## üå≤ Model 3: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 3: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Random Forest with hyperparameter tuning\n",
    "print(\"\\nüîß Performing hyperparameter tuning (this may take a few minutes)...\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [15, 20, 25, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_random = RandomizedSearchCV(rf_base, rf_params, n_iter=20, cv=5, \n",
    "                               scoring='r2', random_state=42, n_jobs=-1, verbose=1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters found: {rf_random.best_params_}\")\n",
    "print(f\"‚úÖ Best CV R¬≤ score: {rf_random.best_score_:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "rf_model = rf_random.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest\")\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test, rf_results['Predictions_Test'], \"Random Forest\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(feature_importance_rf.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = feature_importance_rf.head(10)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='green', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Random Forest - Top 10 Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Random_Forest_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ebc8c",
   "metadata": {},
   "source": [
    "## üöÄ Model 4: Gradient Boosting Regressor (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 4: GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "print(\"\\nüîß Training Gradient Boosting model...\")\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "gb_results = evaluate_model(gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting\")\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test, gb_results['Predictions_Test'], \"Gradient Boosting\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(feature_importance_gb.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea445a9",
   "metadata": {},
   "source": [
    "## ‚ö° Model 5: XGBoost Regressor (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 5: XGBOOST REGRESSOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"\\nüîß Training XGBoost model...\")\n",
    "    \n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test, xgb_results['Predictions_Test'], \"XGBoost\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': xgb_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "    print(feature_importance_xgb.head(10))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è XGBoost not available. Skipping...\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854bbb1d",
   "metadata": {},
   "source": [
    "## üìä Model Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [lr_results, dt_results, rf_results, gb_results]\n",
    "if XGBOOST_AVAILABLE and xgb_results:\n",
    "    all_results.append(xgb_results)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'Model': r['Model'],\n",
    "    'Train_R¬≤': r['Train_R2'],\n",
    "    'Test_R¬≤': r['Test_R2'],\n",
    "    'Test_RMSE': r['Test_RMSE'],\n",
    "    'Test_MAE': r['Test_MAE'],\n",
    "    'Test_MAPE': r['Test_MAPE']\n",
    "} for r in all_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = comparison_df['Test_R¬≤'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_r2 = comparison_df.loc[best_model_idx, 'Test_R¬≤']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤ Score: {best_r2:.4f}\")\n",
    "print(f\"   Test RMSE: ‚Çπ{comparison_df.loc[best_model_idx, 'Test_RMSE']:.2f} Lakhs\")\n",
    "print(f\"   Test MAE: ‚Çπ{comparison_df.loc[best_model_idx, 'Test_MAE']:.2f} Lakhs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c01c93",
   "metadata": {},
   "source": [
    "## üìä Visualization: Model Comparison Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c54905",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. R¬≤ Score Comparison\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "axes[0, 0].bar(x_pos - 0.2, comparison_df['Train_R¬≤'], 0.4, label='Train R¬≤', \n",
    "              color='skyblue', edgecolor='black')\n",
    "axes[0, 0].bar(x_pos + 0.2, comparison_df['Test_R¬≤'], 0.4, label='Test R¬≤', \n",
    "              color='orange', edgecolor='black')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(comparison_df['Model'], rotation=15)\n",
    "axes[0, 0].set_ylabel('R¬≤ Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Model Comparison: R¬≤ Scores', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "axes[0, 1].bar(comparison_df['Model'], comparison_df['Test_RMSE'], \n",
    "              color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_ylabel('RMSE (Lakhs)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Model Comparison: RMSE', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "for i, v in enumerate(comparison_df['Test_RMSE']):\n",
    "    axes[0, 1].text(i, v + 0.5, f'‚Çπ{v:.2f}', ha='center', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. MAE Comparison\n",
    "axes[1, 0].bar(comparison_df['Model'], comparison_df['Test_MAE'], \n",
    "              color='teal', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('MAE (Lakhs)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Model Comparison: MAE', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "for i, v in enumerate(comparison_df['Test_MAE']):\n",
    "    axes[1, 0].text(i, v + 0.3, f'‚Çπ{v:.2f}', ha='center', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. MAPE Comparison\n",
    "axes[1, 1].bar(comparison_df['Model'], comparison_df['Test_MAPE'], \n",
    "              color='purple', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_ylabel('MAPE (%)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Model Comparison: MAPE', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "for i, v in enumerate(comparison_df['Test_MAPE']):\n",
    "    axes[1, 1].text(i, v + 0.5, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Model_Comparison_Charts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235fc10",
   "metadata": {},
   "source": [
    "## üíæ Save Best Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model object\n",
    "model_objects = {\n",
    "    'Linear Regression': lr_model,\n",
    "    'Decision Tree': dt_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'Gradient Boosting': gb_model\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE and xgb_results:\n",
    "    model_objects['XGBoost'] = xgb_model\n",
    "\n",
    "best_model_obj = model_objects[best_model_name]\n",
    "\n",
    "# Save best model\n",
    "model_filename = f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model_obj, f)\n",
    "print(f\"\\nüíæ Best model saved: {model_filename}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_filename = 'feature_scaler.pkl'\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"üíæ Scaler saved: {scaler_filename}\")\n",
    "\n",
    "# Save feature columns\n",
    "features_filename = 'feature_columns.pkl'\n",
    "with open(features_filename, 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)\n",
    "print(f\"üíæ Feature columns saved: {features_filename}\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(f\"üíæ Comparison results saved: model_comparison_results.csv\")\n",
    "\n",
    "# Create model info dictionary\n",
    "model_info = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'test_r2_score': best_r2,\n",
    "    'test_rmse': comparison_df.loc[best_model_idx, 'Test_RMSE'],\n",
    "    'test_mae': comparison_df.loc[best_model_idx, 'Test_MAE'],\n",
    "    'test_mape': comparison_df.loc[best_model_idx, 'Test_MAPE'],\n",
    "    'feature_columns': feature_columns,\n",
    "    'target_column': target_column,\n",
    "    'training_samples': len(X_train),\n",
    "    'testing_samples': len(X_test)\n",
    "}\n",
    "\n",
    "with open('model_info.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "print(f\"üíæ Model info saved: model_info.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL MODELS TRAINED AND COMPARED SUCCESSFULLY!\")\n",
    "print(f\"\\nüèÜ Final Recommendation: Use {best_model_name} for predictions\")\n",
    "print(f\"   Accuracy (R¬≤): {best_r2:.2%}\")\n",
    "print(f\"   Average Error: ¬±‚Çπ{comparison_df.loc[best_model_idx, 'Test_MAE']:.2f} Lakhs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
